{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Twitter Sentimental Analysis using Machine Learning Algorithms**","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Sentiment analysis refers to identifying as well as classifying the sentiments that are expressed in the text source. Tweets are often useful in generating a vast amount of sentiment data upon analysis. These data are useful in understanding the opinion of the people about a variety of topics.\n\nTherefore we need to develop an **Automated Machine Learning Sentiment Analysis Model** in order to compute the customer perception. Due to the presence of non-useful characters (collectively termed as the noise) along with useful data, it becomes difficult to implement models on them.\n\nIn this article, we aim to analyze the sentiment of the tweets provided from **the Sentiment140 dataset** by developing a machine learning pipeline involving the use of three classifiers **(Logistic Regression, Bernoulli Naive Bayes, and SVM)** along with using **Term Frequency- Inverse Document Frequency (TF-IDF)**. The performance of these classifiers is then evaluated using **accuracy** and **F1 Scores.**","metadata":{}},{"cell_type":"markdown","source":"# Problem Statement","metadata":{}},{"cell_type":"markdown","source":"In this project, we try to implement a **Twitter sentiment analysis** model that helps to overcome the challenges of identifying the sentiments of the tweets. The necessary details regarding the dataset are:\n\nThe dataset provided is the **Sentiment Dataset** which consists of 50,000 tweets that have been extracted using the Twitter API. The various columns present in the dataset are:\n\n* **target**: the polarity of the tweet (positive or negative)\n* **ids**: Unique id of the tweet\n* **date**: the date of the tweet\n* **flag**: It refers to the query. If no such query exists then it is NO QUERY.\n* **user**: It refers to the name of the user that tweeted\n* **text**: It refers to the text of the tweet","metadata":{}},{"cell_type":"markdown","source":"# Project pipeline","metadata":{}},{"cell_type":"markdown","source":"The various steps involved in the **Machine Learning Pipeline** are :\n\n* Import Necessary Dependencies\n* Read and Load the Dataset\n* Exploratory Data Analysis\n* Data Visualization of Target Variables\n* Data Preprocessing\n* Splitting our data into Train and Test Subset\n* Transforming Dataset using TF-IDF Vectorizer\n* Function for Model Evaluation\n* Model Building\n* Conclusion","metadata":{}},{"cell_type":"markdown","source":"# Import Necessary Dependencies","metadata":{}},{"cell_type":"code","source":"# utilities\nimport re\nimport numpy as np\nimport pandas as pd\n# plotting\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n# nltk\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n# sklearn\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport warnings \npd.set_option(\"display.max_colwidth\", 200) \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.245344Z","iopub.execute_input":"2022-07-17T16:30:25.245804Z","iopub.status.idle":"2022-07-17T16:30:25.255401Z","shell.execute_reply.started":"2022-07-17T16:30:25.245769Z","shell.execute_reply":"2022-07-17T16:30:25.254171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Read and Load the Dataset**","metadata":{}},{"cell_type":"code","source":"df  = pd.read_csv('../input/twitter-dinesh-train/train_av.csv') \ndftest = pd.read_csv('../input/twitter-dinesh-train/test_av1.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.258070Z","iopub.execute_input":"2022-07-17T16:30:25.258554Z","iopub.status.idle":"2022-07-17T16:30:25.367092Z","shell.execute_reply.started":"2022-07-17T16:30:25.258489Z","shell.execute_reply":"2022-07-17T16:30:25.365612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Exploratory Data Analysis **","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.368575Z","iopub.execute_input":"2022-07-17T16:30:25.368916Z","iopub.status.idle":"2022-07-17T16:30:25.383251Z","shell.execute_reply.started":"2022-07-17T16:30:25.368886Z","shell.execute_reply":"2022-07-17T16:30:25.382203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['label'] == 0].head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.384888Z","iopub.execute_input":"2022-07-17T16:30:25.385538Z","iopub.status.idle":"2022-07-17T16:30:25.405031Z","shell.execute_reply.started":"2022-07-17T16:30:25.385480Z","shell.execute_reply":"2022-07-17T16:30:25.403166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['label'] == 1].head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.408752Z","iopub.execute_input":"2022-07-17T16:30:25.409755Z","iopub.status.idle":"2022-07-17T16:30:25.427085Z","shell.execute_reply.started":"2022-07-17T16:30:25.409714Z","shell.execute_reply":"2022-07-17T16:30:25.425404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.428312Z","iopub.execute_input":"2022-07-17T16:30:25.428961Z","iopub.status.idle":"2022-07-17T16:30:25.442186Z","shell.execute_reply.started":"2022-07-17T16:30:25.428917Z","shell.execute_reply":"2022-07-17T16:30:25.440663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Columns/features in data**","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.443720Z","iopub.execute_input":"2022-07-17T16:30:25.444070Z","iopub.status.idle":"2022-07-17T16:30:25.454250Z","shell.execute_reply.started":"2022-07-17T16:30:25.444039Z","shell.execute_reply":"2022-07-17T16:30:25.453335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('length of data is', len(df)) #1048576","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.455280Z","iopub.execute_input":"2022-07-17T16:30:25.455995Z","iopub.status.idle":"2022-07-17T16:30:25.466497Z","shell.execute_reply.started":"2022-07-17T16:30:25.455961Z","shell.execute_reply":"2022-07-17T16:30:25.465073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df. shape","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.468644Z","iopub.execute_input":"2022-07-17T16:30:25.468984Z","iopub.status.idle":"2022-07-17T16:30:25.479724Z","shell.execute_reply.started":"2022-07-17T16:30:25.468955Z","shell.execute_reply":"2022-07-17T16:30:25.478454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.481326Z","iopub.execute_input":"2022-07-17T16:30:25.481698Z","iopub.status.idle":"2022-07-17T16:30:25.501854Z","shell.execute_reply.started":"2022-07-17T16:30:25.481665Z","shell.execute_reply":"2022-07-17T16:30:25.500727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.503576Z","iopub.execute_input":"2022-07-17T16:30:25.504605Z","iopub.status.idle":"2022-07-17T16:30:25.512161Z","shell.execute_reply.started":"2022-07-17T16:30:25.504568Z","shell.execute_reply":"2022-07-17T16:30:25.510952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sum(df.isnull().any(axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.514188Z","iopub.execute_input":"2022-07-17T16:30:25.515094Z","iopub.status.idle":"2022-07-17T16:30:25.531949Z","shell.execute_reply.started":"2022-07-17T16:30:25.515046Z","shell.execute_reply":"2022-07-17T16:30:25.530577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Count of columns in the data is:  ', len(df.columns))\nprint('Count of rows in the data is:  ', len(df))","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.534032Z","iopub.execute_input":"2022-07-17T16:30:25.534410Z","iopub.status.idle":"2022-07-17T16:30:25.540558Z","shell.execute_reply.started":"2022-07-17T16:30:25.534379Z","shell.execute_reply":"2022-07-17T16:30:25.539364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['label'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.548291Z","iopub.execute_input":"2022-07-17T16:30:25.549560Z","iopub.status.idle":"2022-07-17T16:30:25.561017Z","shell.execute_reply.started":"2022-07-17T16:30:25.549491Z","shell.execute_reply":"2022-07-17T16:30:25.559794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['label'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.563110Z","iopub.execute_input":"2022-07-17T16:30:25.563658Z","iopub.status.idle":"2022-07-17T16:30:25.575850Z","shell.execute_reply.started":"2022-07-17T16:30:25.563609Z","shell.execute_reply":"2022-07-17T16:30:25.574280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dftest.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.578058Z","iopub.execute_input":"2022-07-17T16:30:25.578753Z","iopub.status.idle":"2022-07-17T16:30:25.593365Z","shell.execute_reply.started":"2022-07-17T16:30:25.578701Z","shell.execute_reply":"2022-07-17T16:30:25.591296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('length of data is', len(dftest)) #1048576","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.594740Z","iopub.execute_input":"2022-07-17T16:30:25.595479Z","iopub.status.idle":"2022-07-17T16:30:25.604080Z","shell.execute_reply.started":"2022-07-17T16:30:25.595398Z","shell.execute_reply":"2022-07-17T16:30:25.602933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dftest. shape","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.606058Z","iopub.execute_input":"2022-07-17T16:30:25.606453Z","iopub.status.idle":"2022-07-17T16:30:25.617688Z","shell.execute_reply.started":"2022-07-17T16:30:25.606422Z","shell.execute_reply":"2022-07-17T16:30:25.616731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dftest.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.619295Z","iopub.execute_input":"2022-07-17T16:30:25.619680Z","iopub.status.idle":"2022-07-17T16:30:25.638731Z","shell.execute_reply.started":"2022-07-17T16:30:25.619648Z","shell.execute_reply":"2022-07-17T16:30:25.637787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Plotting data**","metadata":{}},{"cell_type":"code","source":"cnt1 = len(df[df['label']==1])\ncnt0 = len(df[df['label']==0])\npct0 = cnt0/(cnt1+cnt0)\nprint('percent 0',round(pct0*100,2))\npct1 = cnt1/(cnt0+cnt1)\nprint('percent 1',round(pct1*100,2))\nprint(cnt1)\nprint(cnt0)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.639987Z","iopub.execute_input":"2022-07-17T16:30:25.640669Z","iopub.status.idle":"2022-07-17T16:30:25.653439Z","shell.execute_reply.started":"2022-07-17T16:30:25.640635Z","shell.execute_reply":"2022-07-17T16:30:25.651901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the train dataset, we have 2,242 (~7%) tweets labeled as racist or sexist, and 29,720 (~93%) tweets labeled as non racist/sexist. So, it is an imbalanced classification challenge.\n","metadata":{}},{"cell_type":"code","source":"ax=pd.value_counts(df['label']).plot.bar()\n#for multiple colours with tweet data\n#ax = df.groupby('label').count().plot(kind='bar', title='Distribution of data',legend=False)\nplt.title('Distribution')\nax.set_xticklabels(['Negative','Positive'], rotation=0)\nplt.xlabel('label')\nplt.ylabel('counts')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.656740Z","iopub.execute_input":"2022-07-17T16:30:25.658439Z","iopub.status.idle":"2022-07-17T16:30:25.855703Z","shell.execute_reply.started":"2022-07-17T16:30:25.658394Z","shell.execute_reply":"2022-07-17T16:30:25.854675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=df[['tweet','label']]\ndata['label'] = data['label'].replace(4,1)\ndata['label'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.856788Z","iopub.execute_input":"2022-07-17T16:30:25.857786Z","iopub.status.idle":"2022-07-17T16:30:25.873828Z","shell.execute_reply.started":"2022-07-17T16:30:25.857726Z","shell.execute_reply":"2022-07-17T16:30:25.872304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nNow we will check the distribution of length of the tweets, in terms of words, in both train and test data.","metadata":{}},{"cell_type":"code","source":"plt.hist(df.tweet.str.len(), bins=20, label='train')\nplt.hist(dftest.tweet.str.len(), bins=20, label='test')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:25.875289Z","iopub.execute_input":"2022-07-17T16:30:25.875754Z","iopub.status.idle":"2022-07-17T16:30:26.213375Z","shell.execute_reply.started":"2022-07-17T16:30:25.875691Z","shell.execute_reply":"2022-07-17T16:30:26.212192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# get a word count per sentence column\ndef word_count(sentence):\n    return len(sentence.split())\n    \ndf['word count'] = df['tweet'].apply(word_count)\ndf.head(3)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:26.214964Z","iopub.execute_input":"2022-07-17T16:30:26.215364Z","iopub.status.idle":"2022-07-17T16:30:26.287990Z","shell.execute_reply.started":"2022-07-17T16:30:26.215321Z","shell.execute_reply":"2022-07-17T16:30:26.286406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us plot a bar graph positive and negative tweets according word count in the tweet","metadata":{}},{"cell_type":"code","source":"x = df['word count'][df.label == 1]\ny = df['word count'][df.label == 0]\nplt.figure(figsize=(12,6))\nplt.xlim(0,45)\nplt.xlabel('word count')\nplt.ylabel('frequency')\ng = plt.hist([x, y], color=['r','b'], alpha=0.5, label=['positive','negative'])\nplt.legend(loc='upper right')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:26.289674Z","iopub.execute_input":"2022-07-17T16:30:26.290166Z","iopub.status.idle":"2022-07-17T16:30:26.560024Z","shell.execute_reply.started":"2022-07-17T16:30:26.290118Z","shell.execute_reply":"2022-07-17T16:30:26.558876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Here we clearly observe that the maximum tweets are in between 5-25 word.","metadata":{}},{"cell_type":"markdown","source":"In any natural language processing task, cleaning raw text data is an important step. It helps in getting rid of the unwanted words and characters which helps in obtaining better features. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don’t carry much weightage in context to the text.","metadata":{}},{"cell_type":"markdown","source":"#  **Understanding the impact of Hashtags on tweets sentiment**","metadata":{}},{"cell_type":"code","source":"combi = df.append(df, ignore_index=True, sort=True)\ncombi.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:26.561992Z","iopub.execute_input":"2022-07-17T16:30:26.562375Z","iopub.status.idle":"2022-07-17T16:30:26.573740Z","shell.execute_reply.started":"2022-07-17T16:30:26.562343Z","shell.execute_reply":"2022-07-17T16:30:26.572584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hashtags in twitter are synonymous with the ongoing trends on twitter at any particular point in time. We should try to check whether these hashtags add any value to our sentiment analysis task, i.e., they help in distinguishing tweets into the different sentiments.\n\nFor instance, given below is a tweet from our dataset:\n\n![image.png](attachment:a0e14619-2f6a-4a1d-bad0-4f251a377098.png)\n\nThe tweet seems sexist in nature and the hashtags in the tweet convey the same feeling.\n\nWe will store all the trend terms in two separate lists — one for non-racist/sexist tweets and the other for racist/sexist tweets.","metadata":{},"attachments":{"a0e14619-2f6a-4a1d-bad0-4f251a377098.png":{"image/png":"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"}}},{"cell_type":"code","source":"def hashtag_extract(x):\n    hashtags = []    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n    return hashtags","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:26.575911Z","iopub.execute_input":"2022-07-17T16:30:26.576378Z","iopub.status.idle":"2022-07-17T16:30:26.583058Z","shell.execute_reply.started":"2022-07-17T16:30:26.576336Z","shell.execute_reply":"2022-07-17T16:30:26.581971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df.append(dftest,ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:26.584077Z","iopub.execute_input":"2022-07-17T16:30:26.584442Z","iopub.status.idle":"2022-07-17T16:30:26.598794Z","shell.execute_reply.started":"2022-07-17T16:30:26.584404Z","shell.execute_reply":"2022-07-17T16:30:26.597430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extracting hashtags from non racist/sexist tweets \n\nHT_regular = hashtag_extract(data['tweet'][data['label'] == 0])","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:26.600325Z","iopub.execute_input":"2022-07-17T16:30:26.600806Z","iopub.status.idle":"2022-07-17T16:30:26.702164Z","shell.execute_reply.started":"2022-07-17T16:30:26.600773Z","shell.execute_reply":"2022-07-17T16:30:26.700685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extracting hashtags from racist/sexist tweets \n\nHT_negative = hashtag_extract(combi['tweet'][combi['label'] == 1])","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:26.703981Z","iopub.execute_input":"2022-07-17T16:30:26.704361Z","iopub.status.idle":"2022-07-17T16:30:26.730548Z","shell.execute_reply.started":"2022-07-17T16:30:26.704330Z","shell.execute_reply":"2022-07-17T16:30:26.729056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unnesting list\n\nHT_regular = sum(HT_regular,[]) \nHT_negative = sum(HT_negative,[])","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:26.731786Z","iopub.execute_input":"2022-07-17T16:30:26.732180Z","iopub.status.idle":"2022-07-17T16:30:34.144919Z","shell.execute_reply.started":"2022-07-17T16:30:26.732147Z","shell.execute_reply":"2022-07-17T16:30:34.143395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Non-Racist/Sexist Tweets**","metadata":{}},{"cell_type":"markdown","source":"Now that we have prepared our lists of hashtags for both the sentiments, we can plot the top ‘n’ hashtags. So, first let’s check the hashtags in the non-racist/sexist tweets.","metadata":{}},{"cell_type":"code","source":"a = nltk.FreqDist(HT_regular)\nd = pd.DataFrame(\n    {\n    'Hashtag': list(a.keys()),\n    'Count': list(a.values())\n    }\n) \nprint(a)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:34.147085Z","iopub.execute_input":"2022-07-17T16:30:34.147644Z","iopub.status.idle":"2022-07-17T16:30:34.240020Z","shell.execute_reply.started":"2022-07-17T16:30:34.147609Z","shell.execute_reply":"2022-07-17T16:30:34.238589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selecting top 20 most frequent hashtags\n\nd = d.nlargest(columns=\"Count\", n = 20)\nplt.figure(figsize=(20,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\n# plt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:34.242000Z","iopub.execute_input":"2022-07-17T16:30:34.242836Z","iopub.status.idle":"2022-07-17T16:30:34.598863Z","shell.execute_reply.started":"2022-07-17T16:30:34.242793Z","shell.execute_reply":"2022-07-17T16:30:34.597619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Racist/Sexist Tweets**","metadata":{}},{"cell_type":"markdown","source":"All these hashtags are positive and it makes sense. I am expecting negative terms in the plot of the second list. Let’s check the most frequent hashtags appearing in the racist/sexist tweets.","metadata":{}},{"cell_type":"code","source":"b = nltk.FreqDist(HT_negative)\nd = pd.DataFrame(\n    {\n    'Hashtag': list(b.keys()),\n    'Count': list(b.values())\n    }\n) \nprint(a)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:34.602400Z","iopub.execute_input":"2022-07-17T16:30:34.602784Z","iopub.status.idle":"2022-07-17T16:30:34.622262Z","shell.execute_reply.started":"2022-07-17T16:30:34.602755Z","shell.execute_reply":"2022-07-17T16:30:34.620839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selecting top 20 most frequent hashtags\n\nd = d.nlargest(columns=\"Count\", n = 20)\nplt.figure(figsize=(20,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\n# plt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:34.623873Z","iopub.execute_input":"2022-07-17T16:30:34.624210Z","iopub.status.idle":"2022-07-17T16:30:34.957050Z","shell.execute_reply.started":"2022-07-17T16:30:34.624182Z","shell.execute_reply":"2022-07-17T16:30:34.955330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, most of the terms are negative with a few neutral terms as well. So, it’s not a bad idea to keep these hashtags in our data as they contain useful information. Next, we will try to extract features from the tokenized tweets.","metadata":{}},{"cell_type":"markdown","source":"# **CLEANING TWEET DATA**","metadata":{}},{"cell_type":"markdown","source":"Given below is a user-defined function to remove unwanted text patterns from the tweets.","metadata":{}},{"cell_type":"markdown","source":"Before we begin cleaning, let’s first combine train and test datasets. Combining the datasets will make it convenient for us to preprocess the data. Later we will split it back into train and test data.","metadata":{}},{"cell_type":"code","source":"combi = df.append(df, ignore_index=True, sort=True)\ncombi.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:34.959309Z","iopub.execute_input":"2022-07-17T16:30:34.960272Z","iopub.status.idle":"2022-07-17T16:30:34.971095Z","shell.execute_reply.started":"2022-07-17T16:30:34.960233Z","shell.execute_reply":"2022-07-17T16:30:34.969681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n    return input_txt","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:34.972727Z","iopub.execute_input":"2022-07-17T16:30:34.973437Z","iopub.status.idle":"2022-07-17T16:30:34.981536Z","shell.execute_reply.started":"2022-07-17T16:30:34.973401Z","shell.execute_reply":"2022-07-17T16:30:34.979566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be following the steps below to clean the raw tweets in out data.\n\nWe will remove the twitter handles as they are already masked as @user due to privacy concerns. These twitter handles hardly give any information about the nature of the tweet.\n\nWe will also get rid of the punctuations, numbers and even special characters since they wouldn’t help in differentiating different types of tweets.\n\nMost of the smaller words do not add much value. For example, ‘pdx’, ‘his’, ‘all’. So, we will try to remove them as well from our data.\n\nLastly, we will normalize the text data. For example, reducing terms like loves, loving, and lovable to their base word, i.e., ‘love’.are often used in the same context. If we can reduce them to their root word, which is ‘love’. It will help in reducing the total number of unique words in our data without losing a significant amount of information.","metadata":{}},{"cell_type":"markdown","source":"**Removing Twitter Handles (@user)**","metadata":{}},{"cell_type":"markdown","source":"Let’s create a new column tidy_tweet, it will contain the cleaned and processed tweets. Note that we have passed “@[]*” as the pattern to the remove_pattern function. It is actually a regular expression which will pick any word starting with ‘@’.","metadata":{}},{"cell_type":"code","source":"combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\") \ncombi.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:34.983961Z","iopub.execute_input":"2022-07-17T16:30:34.984594Z","iopub.status.idle":"2022-07-17T16:30:35.352789Z","shell.execute_reply.started":"2022-07-17T16:30:34.984543Z","shell.execute_reply":"2022-07-17T16:30:35.351338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing Punctuations, Numbers, and Special Characters**","metadata":{}},{"cell_type":"markdown","source":"Here we will replace everything except characters with spaces. The regular expression “[^a-zA-Z]” means anything except alphabets.","metadata":{}},{"cell_type":"code","source":"combi.tidy_tweet = combi.tidy_tweet.str.replace(\"[^a-zA-Z]\", \" \")\ncombi.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:35.354352Z","iopub.execute_input":"2022-07-17T16:30:35.355033Z","iopub.status.idle":"2022-07-17T16:30:36.134444Z","shell.execute_reply.started":"2022-07-17T16:30:35.354996Z","shell.execute_reply":"2022-07-17T16:30:36.133585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing Short Words**","metadata":{}},{"cell_type":"markdown","source":"We have to be a little careful here in selecting the length of the words which we want to remove. So, I have decided to remove all the words having length 3 or less. For example, terms like “hmm”, “oh” are of very little use. It is better to get rid of them.","metadata":{}},{"cell_type":"code","source":"combi.tidy_tweet = combi.tidy_tweet.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\ncombi.head(10)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:36.135879Z","iopub.execute_input":"2022-07-17T16:30:36.136172Z","iopub.status.idle":"2022-07-17T16:30:36.385360Z","shell.execute_reply.started":"2022-07-17T16:30:36.136145Z","shell.execute_reply":"2022-07-17T16:30:36.384009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see the difference between the raw tweets and the cleaned tweets (tidy_tweet) quite clearly. Only the important words in the tweets have been retained and the noise (numbers, punctuations, and special characters) has been removed.","metadata":{}},{"cell_type":"markdown","source":"# **Text Normalization**","metadata":{}},{"cell_type":"markdown","source":"Here we will use nltk’s PorterStemmer() function to normalize the tweets. But before that we will have to tokenize the tweets. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.","metadata":{}},{"cell_type":"code","source":"tokenized_tweet = combi.tidy_tweet.apply(lambda x: x.split())\ntokenized_tweet.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:36.395202Z","iopub.execute_input":"2022-07-17T16:30:36.395659Z","iopub.status.idle":"2022-07-17T16:30:36.737248Z","shell.execute_reply.started":"2022-07-17T16:30:36.395625Z","shell.execute_reply":"2022-07-17T16:30:36.735252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we can normalize the tokenized tweets.\n\nfrom nltk.stem.porter import * \nstemmer = PorterStemmer() \ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\ntokenized_tweet.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:36.738861Z","iopub.execute_input":"2022-07-17T16:30:36.739305Z","iopub.status.idle":"2022-07-17T16:30:52.369140Z","shell.execute_reply.started":"2022-07-17T16:30:36.739271Z","shell.execute_reply":"2022-07-17T16:30:52.367959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let’s stitch these tokens back together. It can easily be done using nltk’s MosesDetokenizer function.\n\nfor i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])    \ncombi['tidy_tweet'] = tokenized_tweet\ncombi.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:52.371390Z","iopub.execute_input":"2022-07-17T16:30:52.372314Z","iopub.status.idle":"2022-07-17T16:30:53.192390Z","shell.execute_reply.started":"2022-07-17T16:30:52.372275Z","shell.execute_reply":"2022-07-17T16:30:53.191044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this section, we will explore the cleaned tweets. Exploring and visualizing data, no matter whether its text or any other data, is an essential step in gaining insights. Do not limit yourself to only these methods told in this course, feel free to explore the data as much as possible.\n\nBefore we begin exploration, we must think and ask questions related to the data in hand. A few probable questions are as follows:\n\n* What are the most common words in the entire dataset?\n* What are the most common words in the dataset for negative and positive tweets, respectively?\n* How many hashtags are there in a tweet?\n* Which trends are associated with my dataset?\n* Which trends are associated with either of the sentiments? Are they compatible with the sentiments?","metadata":{}},{"cell_type":"markdown","source":"# **WORD CLOUD**","metadata":{}},{"cell_type":"markdown","source":"**Understanding the common words used in the tweets: WordCloud**","metadata":{}},{"cell_type":"markdown","source":"Now I want to see how well the given sentiments are distributed across the train dataset. One way to accomplish this task is by understanding the common words by plotting wordclouds.\n\nA wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.\n\nLet’s visualize all the words our data using the wordcloud plot.","metadata":{}},{"cell_type":"markdown","source":"**plot a cloud of word for top 100 words in the tweets**","metadata":{}},{"cell_type":"code","source":"all_words = ' '.join([str(text) for text in combi['tidy_tweet']])\n\nfrom wordcloud import WordCloud,STOPWORDS\n#stopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n    width=800, \n    height=500,\n    #stopwords=stopwords,\n    background_color='white',\n    random_state=21, \n    max_font_size=130,#maximum fount size in word cloud\n    max_words=100 #no. of word can be place there\n).generate(all_words) \nplt.figure(figsize=(15,15)) #size of word cloud\nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:53.194661Z","iopub.execute_input":"2022-07-17T16:30:53.195044Z","iopub.status.idle":"2022-07-17T16:30:57.395999Z","shell.execute_reply.started":"2022-07-17T16:30:53.195014Z","shell.execute_reply":"2022-07-17T16:30:57.395079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see most of the words are positive or neutral. Words like love, great, friend, life are the most frequent ones. It doesn’t give us any idea about the words associated with the racist/sexist tweets. Hence, we will plot separate wordclouds for both the classes (racist/sexist or not) in our train data.","metadata":{}},{"cell_type":"markdown","source":"**plot a cloud of word for all the words in the tweets**","metadata":{}},{"cell_type":"code","source":"all_words1 = ' '.join([str(text) for text in combi['tidy_tweet']])\n\nfrom wordcloud import WordCloud,STOPWORDS\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n    width=800, \n    height=500,\n    stopwords=stopwords,\n    background_color='white',\n    random_state=21, \n    max_font_size=130,#maximum fount size in word cloud\n    #max_words=100 #no. of word can be place there\n).generate(all_words1) \nplt.figure(figsize=(15,15)) #size of word cloud\nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:30:57.397286Z","iopub.execute_input":"2022-07-17T16:30:57.398298Z","iopub.status.idle":"2022-07-17T16:31:01.833894Z","shell.execute_reply.started":"2022-07-17T16:30:57.398255Z","shell.execute_reply":"2022-07-17T16:31:01.832689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see most of the words are positive or neutral. Words like love, great, friend, life are the most frequent ones. It doesn’t give us any idea about the words associated with the racist/sexist tweets. Hence, we will plot separate wordclouds for both the classes (racist/sexist or not) in our train data.","metadata":{}},{"cell_type":"markdown","source":"**Plot a cloud of words for positive tweets**","metadata":{}},{"cell_type":"code","source":"Postive_tweet =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]]) \nfrom wordcloud import WordCloud,STOPWORDS\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n    width=800, \n    height=500,\n    stopwords=stopwords,\n    background_color='white',\n    random_state=21, \n    max_font_size=130,#maximum fount size in word cloud\n    #max_words=100 #no. of word can be place there\n).generate(Postive_tweet) \nplt.figure(figsize=(15,15)) #size of word cloud\nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:01.835408Z","iopub.execute_input":"2022-07-17T16:31:01.836724Z","iopub.status.idle":"2022-07-17T16:31:05.977710Z","shell.execute_reply.started":"2022-07-17T16:31:01.836686Z","shell.execute_reply":"2022-07-17T16:31:05.976586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the frequent words are compatible with the sentiment, i.e, non-racist/sexists tweets. Similarly, we will plot the word cloud for the other sentiment. Expect to see negative, racist, and sexist terms.","metadata":{}},{"cell_type":"markdown","source":"**Plot a cloud of words for negative tweets**","metadata":{}},{"cell_type":"code","source":"Negative_tweet =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\nfrom wordcloud import WordCloud,STOPWORDS\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n    width=800, \n    height=500,\n    stopwords=stopwords,\n    background_color='white',\n    random_state=21, \n    max_font_size=130,#maximum fount size in word cloud\n    #max_words=100 #no. of word can be place there\n).generate(Negative_tweet) \nplt.figure(figsize=(15,15)) #size of word cloud\nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:05.978869Z","iopub.execute_input":"2022-07-17T16:31:05.979731Z","iopub.status.idle":"2022-07-17T16:31:07.311700Z","shell.execute_reply.started":"2022-07-17T16:31:05.979698Z","shell.execute_reply":"2022-07-17T16:31:07.310483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can clearly see, most of the words have negative connotations. So, it seems we have a pretty good text data to work on. Next we will the hashtags/trends in our twitter data.","metadata":{}},{"cell_type":"markdown","source":"# **Splitting our data into Train and Test Subset**","metadata":{}},{"cell_type":"code","source":"X=combi.tweet\ny=combi.label","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:07.313494Z","iopub.execute_input":"2022-07-17T16:31:07.314254Z","iopub.status.idle":"2022-07-17T16:31:07.319720Z","shell.execute_reply.started":"2022-07-17T16:31:07.314216Z","shell.execute_reply":"2022-07-17T16:31:07.318142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separating the 95% data for training data and 5% for testing data\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.05, random_state =26105111)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:07.321238Z","iopub.execute_input":"2022-07-17T16:31:07.322049Z","iopub.status.idle":"2022-07-17T16:31:07.340949Z","shell.execute_reply.started":"2022-07-17T16:31:07.322014Z","shell.execute_reply":"2022-07-17T16:31:07.339564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Transforming Dataset using TF-IDF Vectorizer***","metadata":{}},{"cell_type":"markdown","source":"**Fit the TF-IDF Vectorizer**","metadata":{}},{"cell_type":"code","source":"vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:07.343126Z","iopub.execute_input":"2022-07-17T16:31:07.343518Z","iopub.status.idle":"2022-07-17T16:31:10.924486Z","shell.execute_reply.started":"2022-07-17T16:31:07.343468Z","shell.execute_reply":"2022-07-17T16:31:10.923111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transform the data using TF-IDF Vectorizer**","metadata":{}},{"cell_type":"code","source":"X_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:10.926132Z","iopub.execute_input":"2022-07-17T16:31:10.927171Z","iopub.status.idle":"2022-07-17T16:31:13.766596Z","shell.execute_reply.started":"2022-07-17T16:31:10.927120Z","shell.execute_reply":"2022-07-17T16:31:13.765554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Function For Model Evaluation**","metadata":{}},{"cell_type":"markdown","source":"After training the model we then apply the evaluation measures to check how the model is performing. Accordingly, we use the following evaluation parameters to check the performance of the models respectively :\n\n* Accuracy Score\n* Confusion Matrix with Plot\n* ROC-AUC Curve","metadata":{}},{"cell_type":"code","source":"def model_Evaluate(model):\n    # Predict values for Test dataset\n    y_pred= model.predict(X_test)\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    categories = ['Negative','Positive']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n    labels = [f'{v1}n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n    xticklabels = categories, yticklabels = categories)\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\" , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:13.767985Z","iopub.execute_input":"2022-07-17T16:31:13.768398Z","iopub.status.idle":"2022-07-17T16:31:13.779845Z","shell.execute_reply.started":"2022-07-17T16:31:13.768357Z","shell.execute_reply":"2022-07-17T16:31:13.778494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Model Building***","metadata":{}},{"cell_type":"markdown","source":"In the problem statement we have used three different models respectively :\n\n* Bernoulli Naive Bayes\n* SVM (Support Vector Machine)\n* Logistic Regression\n\nThe idea behind choosing these models is that we want to try all the classifiers on the dataset ranging from simple ones to complex models and then try to find out the one which gives the best performance among them.","metadata":{}},{"cell_type":"markdown","source":"**Model1:Bernoulli Naive Bayes model**","metadata":{}},{"cell_type":"code","source":"BNBmodel = BernoulliNB()\nBNBmodel.fit(X_train, y_train)\nmodel_Evaluate(BNBmodel)\ny_pred1 = BNBmodel.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:13.781847Z","iopub.execute_input":"2022-07-17T16:31:13.782182Z","iopub.status.idle":"2022-07-17T16:31:14.115656Z","shell.execute_reply.started":"2022-07-17T16:31:13.782153Z","shell.execute_reply":"2022-07-17T16:31:14.114492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot the ROC-AUC Curve for model-1**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_pred1)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC CURVE')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:14.117166Z","iopub.execute_input":"2022-07-17T16:31:14.118338Z","iopub.status.idle":"2022-07-17T16:31:14.317925Z","shell.execute_reply.started":"2022-07-17T16:31:14.118290Z","shell.execute_reply":"2022-07-17T16:31:14.316383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model2: SVM (Support Vector Machine) model**","metadata":{}},{"cell_type":"code","source":"SVCmodel = LinearSVC()\nSVCmodel.fit(X_train, y_train)\nmodel_Evaluate(SVCmodel)\ny_pred2 = SVCmodel.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:14.321442Z","iopub.execute_input":"2022-07-17T16:31:14.321810Z","iopub.status.idle":"2022-07-17T16:31:15.249336Z","shell.execute_reply.started":"2022-07-17T16:31:14.321780Z","shell.execute_reply":"2022-07-17T16:31:15.248442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Plot the ROC-AUC Curve for model-2***","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_pred2)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC CURVE')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:15.250584Z","iopub.execute_input":"2022-07-17T16:31:15.251156Z","iopub.status.idle":"2022-07-17T16:31:15.439649Z","shell.execute_reply.started":"2022-07-17T16:31:15.251123Z","shell.execute_reply":"2022-07-17T16:31:15.438550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model-3 Logistic Regression model**","metadata":{}},{"cell_type":"code","source":"LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\nLRmodel.fit(X_train, y_train)\nmodel_Evaluate(LRmodel)\ny_pred3 = LRmodel.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:15.441191Z","iopub.execute_input":"2022-07-17T16:31:15.442223Z","iopub.status.idle":"2022-07-17T16:31:21.714278Z","shell.execute_reply.started":"2022-07-17T16:31:15.442173Z","shell.execute_reply":"2022-07-17T16:31:21.713245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot the ROC-AUC Curve for model-3**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_pred3)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC CURVE')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T16:31:21.716067Z","iopub.execute_input":"2022-07-17T16:31:21.717044Z","iopub.status.idle":"2022-07-17T16:31:21.920241Z","shell.execute_reply.started":"2022-07-17T16:31:21.716991Z","shell.execute_reply":"2022-07-17T16:31:21.918749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **conclusion:**","metadata":{}},{"cell_type":"markdown","source":"Upon evaluating all the models we can conclude the following details i.e.\n\n**Accuracy:** As far as the accuracy of the model is concerned Logistic Regression performs better than SVM which in turn performs better than Bernoulli Naive Bayes.\n\n**F1-score:** The F1 Scores for class 0 and class 1 are :\n\n(a) For class 0: Logistic Regression (accuracy = 0.98)< Bernoulli Naive Bayes(accuracy = 0.98) < SVM (accuracy =1.00) \n\n(b) For class 1: Logistic Regression (accuracy = 0.65)< Bernoulli Naive Bayes (accuracy = 0.69) < SVM (accuracy = 0.98) \n\n**AUC Score:** All three models have the same ROC-AUC score.\n\nWe, therefore, conclude that the SVM is the best model for the above-given dataset.\n\nIn our problem statement, **SVM Support-vector machine** is following the principle of **Occam’s Razor** which defines that for a particular problem statement if the data has no assumption, then the simplest model works the best. Since our dataset does not have any assumptions and SVM is a simple model, therefore the concept holds true for the above-mentioned dataset","metadata":{}}]}